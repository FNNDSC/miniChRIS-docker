# an ephemeral instance of ChRIS backend services for local development
#
# warning: /var/run/docker.sock is mounted into some services (notably pman)

services:
  chrisomatic:
    image: ghcr.io/fnndsc/chrisomatic:1.0.0
    profiles:
      - tools
    volumes:
      - "./chrisomatic.yml:/chrisomatic.yml:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:rw"
    userns_mode: host
    depends_on:
      - chris
    networks:
      - local

  db_migrate:
    image: ghcr.io/fnndsc/cube:6.3.3
    command: python manage.py migrate --noinput
    env_file: secrets.env
    volumes:
      - chris_files:/data:rw
    depends_on:
      db:
        condition: service_healthy
    networks:
      - local

  chris:
    container_name: chris
    image: ghcr.io/fnndsc/cube:6.3.3
    ports:
      - "8000:8000"
    volumes:
      - chris_files:/data:rw
    depends_on:
      db_migrate:
        condition: service_completed_successfully
      queue:
        condition: service_healthy
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully
    networks:
      - local
    env_file: secrets.env
    labels:
      org.chrisproject.role: "ChRIS_ultron_backEnd"
      org.chrisproject.miniChRIS: "miniChRIS"
  worker:
    image: ghcr.io/fnndsc/cube:6.3.3
    command: celery -A core worker -c 4 -l info -Q main1,main2
    volumes:
      - chris_files:/data:rw
    env_file: secrets.env
    depends_on:
      db_migrate:
        condition: service_completed_successfully
      queue:
        condition: service_healthy
      pfcon:
        condition: service_started
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - local
  worker_periodic:
    image: ghcr.io/fnndsc/cube:6.3.3
    command: celery -A core worker -c 2 -l info -Q periodic
    volumes:
      - chris_files:/data:rw
    env_file: secrets.env
    depends_on:
      db_migrate:
        condition: service_completed_successfully
      queue:
        condition: service_healthy
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - local
  scheduler:
    image: ghcr.io/fnndsc/cube:6.3.3
    command: celery -A core beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    volumes:
      - chris_files:/data:rw
    env_file: secrets.env
    depends_on:
      db_migrate:
        condition: service_completed_successfully
      queue:
        condition: service_healthy
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - local
  db:
    image: docker.io/library/postgres:17
    env_file: secrets.env
    restart: unless-stopped
    volumes:
      - db_data:/var/lib/postgresql/data
    networks:
      - local
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 2s
      timeout: 4s
      retries: 3
      start_period: 60s
  queue:
    image: docker.io/library/rabbitmq:3
    restart: on-failure
    ports:
      - 5672:5672
    networks:
      - local
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      start_period: 20s
      retries: 3
      timeout: 10s
      interval: 5s

  pfcon:
    container_name: pfcon
    image: ghcr.io/fnndsc/pfcon:5.2.3
    environment:
      COMPUTE_SERVICE_URL: http://pman:5010/api/v1/
      SECRET_KEY: secret
      PFCON_USER: pfcon
      PFCON_PASSWORD: pfcon1234
      PFCON_INNETWORK: "true"
      STORAGE_ENV: fslink
      STOREBASE_MOUNT: /var/local/storeBase
    ports:
      - "5005:5005"
    volumes:
      - chris_files:/var/local/storeBase
    networks:
      local:
        aliases:
          - pfcon.host
      remote:
    labels:
      org.chrisproject.role: "pfcon"
    user: "1001"
    depends_on:
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully

  pman:
    image: ghcr.io/fnndsc/pman:6.2.0
    container_name: pman
    environment:
      CONTAINER_ENV: docker
      CONTAINER_USER: "1001:"
      ENABLE_HOME_WORKAROUND: "yes"
      JOB_LABELS: "org.chrisproject.miniChRIS=plugininstance"
      SECRET_KEY: secret
      REMOVE_JOBS: "yes"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
    depends_on:
      # see https://github.com/FNNDSC/pman/blob/v6.2.0/README.md#how-storage-works
      - pfcon
    ports:
      - "5010:5010"
    networks:
      remote:
    userns_mode: "host"
    labels:
      org.chrisproject.role: "pman"

  chris_ui:
    # This image is likely outdated.
    # Newer tags can be found here --> https://github.com/FNNDSC/ChRIS_ui/pkgs/container/chris_ui
    image: ghcr.io/fnndsc/chris_ui:staging
    environment:
      # https://chrisproject.org/docs/run/chris_ui#environment-variables
      CHRIS_UI_URL: http://${HOSTNAME:-localhost}:8000/api/v1/
      PFDCM_URL: http://${HOSTNAME:-localhost}:4005
      OHIF_URL: http://${HOSTNAME:-localhost}:8042/ohif/
      SERVER_PORT: "8020"
      SERVER_LOG_REMOTE_ADDRESS: "true"
      SERVER_HEALTH: "true"
    ports:
      - "8020:8020"
    user: "100100:100100"
    group_add: [ root ]
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://127.0.0.1:8020/health"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 10s

  orthanc:
    image: docker.io/jodogne/orthanc-plugins:1.12.7
    volumes:
      - ./orthanc.json:/etc/orthanc/orthanc.json:ro
      - orthanc:/var/lib/orthanc/db
    ports:
      - "4242:4242"
      - "8042:8042"
    networks:
      - pacs
    profiles:
      - pacs
      - orthanc

  pfdcm:
    image: ghcr.io/fnndsc/pfdcm:3.1.2
    container_name: pfdcm
    environment:
      MAX_WORKERS: 1
    volumes:
      - pfdcm:/home/dicom:rw
      - ./pfdcm-services:/home/dicom/services:ro
      - chris_files:/chris_files:rw
    ports:
      - "4005:4005"
    networks:
      - pacs
    user: "1001"
    profiles:
      - pacs
    depends_on:
      pfdcm-nonroot-user-volume-fix:
        condition: service_completed_successfully

  oxidicom:
    image: ghcr.io/fnndsc/oxidicom:3.0.0
    environment:
      # https://chrisproject.org/docs/oxidicom/deployment#environment-variables
      OXIDICOM_FILES_ROOT: /data
      OXIDICOM_AMQP_ADDRESS: amqp://queue:5672
      OXIDICOM_NATS_ADDRESS: nats:4222
      OXIDICOM_SCP_AET: ChRIS
      OXIDICOM_SCP_PROMISCUOUS: "true"
      OXIDICOM_LISTENER_THREADS: 32
      OXIDICOM_LISTENER_PORT: 11111
      RUST_LOG: oxidicom=info
      OXIDICOM_PROGRESS_INTERVAL: 100ms
      OXIDICOM_DEV_SLEEP: 150ms  # throttle performance, so that we can see what's going on in ChRIS_ui
    ports:
      - "11111:11111"
    volumes:
      - chris_files:/data:rw
    networks:
      - pacs
      - local
    profiles:
      - pacs
    user: 1001:0
    stop_signal: SIGKILL
    depends_on:
      queue:
        condition: service_healthy
      nats:
        condition: service_healthy
      cube-nonroot-user-volume-fix:
        condition: service_completed_successfully

  nats:
    # NOTE: using -alpine instead of -scratch so wget can be used in the healthcheck
    image: docker.io/library/nats:2.11.4-alpine
    ports:
      - "4222:4222"
    networks:
      - local
    profiles:
      - pacs
    healthcheck:
      test: wget http://localhost:8222/healthz -q -S -O -
      start_period: 10s
      retries: 3
      timeout: 2s
      interval: 2s

  pfbridge:
    image: docker.io/fnndsc/pfbridge:3.7.2
    container_name: pfbridge
    environment:
      MAX_WORKERS: 1
      PFLINK_USERNAME: pflink
      PFLINK_PASSWORD: pflink1234
      NAME: PFDCMLOCAL
      PACSNAME: orthanc
      CUBEANDSWIFTKEY: local
    ports:
      - "33333:33333"
    networks:
      local:
      pflink:
    profiles:
      - pflink

  pflink:
    image: docker.io/fnndsc/pflink:settings-39e91ed
    container_name: pflink
    restart: unless-stopped
    environment:
      PFDCM_NAME: "NOTPFDCMLOCAL"  # work around for hard-coded edge case
      PFLINK_MONGODB: "mongodb://pflink-db:27017"
      PFLINK_PFDCM: "http://pfdcm:4005"
      PFLINK_PORT: "4010"
    ports:
      - "4010:4010"
    networks:
      local:
      pflink:
    depends_on:
      - pflink-db
    profiles:
      - pflink

  pflink-db:
    image: mongo
    environment:
      - PUID=1000
      - PGID=1000
    volumes:
      - pflink-db-data:/data/db
    restart: unless-stopped
    networks:
      pflink:
    profiles:
      - pflink

  graphql-engine:
    image: docker.io/hasura/graphql-engine:v2.41.0
    ports:
      - "8090:8080"
    restart: unless-stopped
    environment:
      ## postgres database to store Hasura metadata
      HASURA_GRAPHQL_METADATA_DATABASE_URL: postgres://hasura:hasura1234@hasura-db:5432/hasura
      ## enable the console served by server
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true" # set to "false" to disable console
      ## enable debugging mode. It is recommended to disable this in production
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      ## uncomment next line to run console offline (i.e load console assets from server instead of CDN)
      HASURA_GRAPHQL_CONSOLE_ASSETS_DIR: /srv/console-assets
      ## uncomment next line to set an admin secret
      # HASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey
      HASURA_GRAPHQL_METADATA_DEFAULTS: '{"backend_configs":{"dataconnector":{"athena":{"uri":"http://data-connector-agent:8081/api/v1/athena"},"mariadb":{"uri":"http://data-connector-agent:8081/api/v1/mariadb"},"mysql8":{"uri":"http://data-connector-agent:8081/api/v1/mysql"},"oracle":{"uri":"http://data-connector-agent:8081/api/v1/oracle"},"snowflake":{"uri":"http://data-connector-agent:8081/api/v1/snowflake"}}}}'
    depends_on:
      hasura-db:
        condition: service_healthy
      data-connector-agent:
        condition: service_healthy
    networks:
      local:
    profiles:
      - hasura
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz?strict=false"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 5s
  data-connector-agent:
    image: hasura/graphql-data-connector:v2.40.0
    restart: unless-stopped
    ports:
      - 8081:8081
    environment:
      QUARKUS_LOG_LEVEL: ERROR # FATAL, ERROR, WARN, INFO, DEBUG, TRACE
      ## https://quarkus.io/guides/opentelemetry#configuration-reference
      QUARKUS_OPENTELEMETRY_ENABLED: "false"
      ## QUARKUS_OPENTELEMETRY_TRACER_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/api/v1/athena/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 5s
    networks:
      local:
    profiles:
      - hasura
  hasura-db:
    image: docker.io/library/postgres:15
    restart: unless-stopped
    volumes:
      - hasura-db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: hasura
      POSTGRES_PASSWORD: hasura1234
      POSTGRES_DB: hasura
    networks:
      local:
    profiles:
      - hasura
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 2s
      timeout: 4s
      retries: 3
      start_period: 60s
  hasura-cli:
    image: ghcr.io/fnndsc/hasura-cli:2.41.0
    command: hasura metadata apply
    restart: "no"
    volumes:
      - ./hasura:/hasura:ro
    working_dir: /hasura
    networks:
      local:
    profiles:
      - hasura
    depends_on:
      graphql-engine:
        condition: service_healthy

  # Non-root container user workarounds

  cube-nonroot-user-volume-fix:
    image: docker.io/library/alpine:latest
    volumes:
      - chris_files:/data:rw
    user: root
    command: chmod g+rwx /data
    restart: "no"

  pfdcm-nonroot-user-volume-fix:
    image: docker.io/library/alpine:latest
    volumes:
      - pfdcm:/home/dicom:rw
    user: root
    command: chown 1001 /home/dicom
    restart: "no"

networks:
  local:
    name: minichris-local
  remote:
  pacs:
  monitoring:
  pflink:

volumes:
  chris_files:
    name: minichris-files
  db_data:
  orthanc:
  pfdcm:
  grafana_data:
  openobserve_data:
  pflink-db-data:
  hasura-db-data:
